<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Universal Full‑Stack Agentic Architecture — Maximized (Technique Atlas)</title>
  <style>
    :root{--bg:#0b0f14;--fg:#e9eef5;--muted:#b7c2d0;--card:#111823;--accent:#7dd3fc;--accent2:#a7f3d0;--line:#223043;}
    body{margin:0;font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Arial; background:var(--bg); color:var(--fg); line-height:1.55;}
    header{padding:28px 22px;border-bottom:1px solid var(--line); background:linear-gradient(180deg,#0b0f14,#0b0f14 60%,#0c121b);}
    h1{margin:0 0 8px;font-size:28px;letter-spacing:.2px}
    .sub{color:var(--muted);max-width:1100px}
    main{max-width:1200px;margin:0 auto;padding:18px 22px 60px;}
    .grid{display:grid;grid-template-columns:320px 1fr;gap:18px}
    @media (max-width: 980px){.grid{grid-template-columns:1fr}}
    nav{position:sticky;top:12px;align-self:start;background:var(--card);border:1px solid var(--line);border-radius:14px;padding:14px;max-height:calc(100vh - 24px);overflow:auto}
    nav a{display:block;color:var(--muted);text-decoration:none;padding:6px 8px;border-radius:10px}
    nav a:hover{background:#0c1320;color:var(--fg)}
    section{background:var(--card);border:1px solid var(--line);border-radius:14px;padding:16px 16px 10px;margin-bottom:14px}
    h2{margin:0 0 8px;font-size:18px}
    h3{margin:14px 0 6px;font-size:15px;color:var(--accent)}
    ul{margin:8px 0 14px;padding-left:18px}
    li{margin:4px 0}
    .pill{display:inline-block;border:1px solid var(--line);border-radius:999px;padding:2px 10px;color:var(--muted);font-size:12px;margin-right:6px}
    .callout{border-left:3px solid var(--accent);padding:10px 12px;background:#0c1320;border-radius:10px;margin:10px 0}
    .two{display:grid;grid-template-columns:1fr 1fr;gap:12px}
    @media (max-width: 980px){.two{grid-template-columns:1fr}}
    code, pre{background:#0c1320;border:1px solid var(--line);border-radius:10px}
    code{padding:2px 6px}
    pre{padding:10px 12px;overflow:auto}
    .small{color:var(--muted);font-size:12px}
    .kpi{display:grid;grid-template-columns:repeat(4,1fr);gap:10px;margin:10px 0 4px}
    @media (max-width: 980px){.kpi{grid-template-columns:repeat(2,1fr)}}
    .k{background:#0c1320;border:1px solid var(--line);border-radius:12px;padding:10px}
    .k b{display:block;font-size:13px}
    .k span{color:var(--muted);font-size:12px}
    footer{color:var(--muted);padding:18px 22px;border-top:1px solid var(--line);max-width:1200px;margin:0 auto}
    a{color:var(--accent)}
  </style>
</head>
<body>
<header>
  <h1>Universal Full‑Stack Agentic Architecture — Maximized</h1>
  <div class="sub">
    This is a <b>universal</b> runtime you can reuse for any app: “App = UI wrapper + versioned App Pack”.<br/>
    The entire point is to expose <b>every high‑leverage AI/ML trick</b> as a <b>slot</b> the runtime can choose at run‑time (fast/cheap/accurate/safe).
  </div>
  <div class="kpi">
    <div class="k"><b>Primary KPI</b><span>p95 latency & correctness</span></div>
    <div class="k"><b>Core Mechanism</b><span>cascades + parallel DAG + cancel</span></div>
    <div class="k"><b>Universality</b><span>packs: intents/graphs/tools/skills</span></div>
    <div class="k"><b>Learning Loop</b><span>logs → routers/verifiers/distill</span></div>
  </div>
</header>

<main class="grid">
<nav>
    <div class="pill">Table of Contents</div>
    <a href="#arch">Architecture diagram</a>
    <a href="#big">0. The “Big Knobs” (what you keep asking for)</a>
    <a href="#slots">1. Runtime Slots (where every trick plugs in)</a>
    <a href="#speed">2. Speed Tricks (systems + inference)</a>
    <a href="#retrieval">3. Retrieval & Memory Tricks (beyond basic RAG)</a>
    <a href="#reasoning">4. Reasoning / Test‑time Compute Tricks</a>
    <a href="#routing">5. Routing / Cascades / Budgeting</a>
    <a href="#peft">6. Specialization beyond LoRA (PEFT zoo)</a>
    <a href="#editing">7. Updating Knowledge (model editing vs retrieval)</a>
    <a href="#training">8. Alignment / Preference / Feedback learning</a>
    <a href="#eval">9. Evaluation & Observability (how it keeps getting smarter)</a>
    <a href="#impl">10. Langflow + Cloudflare + HuggingFace mapping</a>
    <a href="#stock">11. StockCommand overlay (example App Pack)</a>
  </nav>

  <div>
    <section id="arch" style="margin-top:16px">
  <h2>Architecture (Full‑Stack Universal Runtime)</h2>
  <div class="callout">
    This diagram is the canonical layout: <b>UI → Edge Gateway → Compiler → Orchestrator → Stores → Model/Tool Plane → Ops/Learning</b>.
    Lines are routed outside boxes to keep the diagram readable.
  </div>
  <div style="border:1px solid var(--line); border-radius:14px; overflow:hidden; background:#0c1320; padding:12px">
    <svg viewBox="0 0 1200 520" role="img" aria-label="Universal Full-Stack Agentic Architecture Diagram" style="display:block;width:100%;height:auto">
      <defs>
        <marker id="arrow" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="8" markerHeight="8" orient="auto-start-reverse">
          <path d="M 0 0 L 10 5 L 0 10 z" fill="#7dd3fc"/>
        </marker>
        <marker id="arrowPurple" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="8" markerHeight="8" orient="auto-start-reverse">
          <path d="M 0 0 L 10 5 L 0 10 z" fill="#a78bfa"/>
        </marker>
        <style>
          .box{fill:#0f1722;stroke:#223043;stroke-width:2}
          .label{fill:#e9eef5;font:600 18px ui-sans-serif,system-ui}
          .sub{fill:#b7c2d0;font:500 13px ui-sans-serif,system-ui}
          .arrow{stroke:#7dd3fc;stroke-width:3;fill:none;marker-end:url(#arrow)}
          .arrow-purple{stroke:#a78bfa;stroke-width:3;fill:none;marker-end:url(#arrowPurple)}
        </style>
      </defs>

      <rect class="box" x="30" y="40" width="200" height="70" rx="12"/>
      <text class="label" x="60" y="82">UI</text>
      <text class="sub" x="60" y="102">Web / Native</text>

      <rect class="box" x="260" y="40" width="210" height="70" rx="12"/>
      <text class="label" x="280" y="82">Edge Gateway</text>
      <text class="sub" x="280" y="102">Auth, rate, stream</text>

      <rect class="box" x="500" y="40" width="210" height="70" rx="12"/>
      <text class="label" x="520" y="82">Compiler</text>
      <text class="sub" x="520" y="102">Langflow → DAG</text>

      <rect class="box" x="740" y="40" width="210" height="70" rx="12"/>
      <text class="label" x="760" y="82">Orchestrator</text>
      <text class="sub" x="760" y="102">Budgets, cancel</text>

      <rect class="box" x="980" y="40" width="190" height="70" rx="12"/>
      <text class="label" x="1000" y="82">Stores</text>
      <text class="sub" x="1000" y="102">KV / R2 / DB</text>

      <rect class="box" x="120" y="160" width="430" height="150" rx="14"/>
      <text class="label" x="150" y="195">Model / Tool Plane</text>
      <text class="sub" x="150" y="220">LLMs • Tools • Retrieval • Diffusion</text>
      <text class="sub" x="150" y="245">Adapters • Quantization • Verifiers</text>

      <rect class="box" x="650" y="160" width="430" height="150" rx="14"/>
      <text class="label" x="680" y="195">Ops + Learning</text>
      <text class="sub" x="680" y="220">Logs • Eval • Distill • Safety</text>
      <text class="sub" x="680" y="245">Batch / Scaling • Monitoring</text>

      <path class="arrow" d="M 230 75 L 260 75"/>
      <path class="arrow" d="M 470 75 L 500 75"/>
      <path class="arrow" d="M 710 75 L 740 75"/>
      <path class="arrow" d="M 950 75 L 980 75"/>

      <path class="arrow" d="M 845 110 L 560 160"/>
      <path class="arrow" d="M 1040 110 L 870 160"/>

      <path class="arrow-purple" d="M 335 310 L 335 420 L 865 420 L 865 310"/>
      <text class="sub" x="360" y="410">Feedback loop: traces → routers/verifiers/distill</text>
    </svg>
  </div>
  </section>

    <section id="big">
      <h2>0) The “Big Shit” (high‑leverage techniques beyond LoRA)</h2>
      <div class="callout">
        If you want a universal agentic framework, you don’t pick 1 technique —
        you build a runtime that can <b>select among dozens</b> per request, using <b>confidence + budgets + verifiers</b>.
        <br/>A determination like that depends on specific goals (latency, cost, reliability, developer ergonomics, safety) and evidence across alternatives.
      </div>

      <h3>Top levers (in practice these dominate latency + quality)</h3>
      <ul>
        <li><b>Routing cascades</b> (cheap → expensive) + cancellation: rules/embeddings/zero‑shot/tiny router/LLM router; plus LLM cascades (FrugalGPT‑style).</li>
        <li><b>Cache pyramid + in‑flight dedupe</b>: avoid LLM calls entirely when possible; share tool calls across concurrent requests.</li>
        <li><b>Prompt/context compression</b>: compress inputs before expensive model calls (e.g., LLMLingua‑style compression).</li>
        <li><b>Retrieval that self‑audits</b>: retrieval evaluators and “retrieve only if needed” (Self‑RAG/CRAG‑style robustness).</li>
        <li><b>Hierarchical retrieval</b> for long corpora: tree‑organized indexes (RAPTOR‑style) or multi‑query fusion (RAG‑Fusion‑style).</li>
        <li><b>Inference acceleration</b>: speculative decoding, batching, KV/prefix caching; prefill/decode disaggregation.</li>
        <li><b>Specialization as skills</b>: PEFT/adapters library (LoRA, QLoRA, IA³, prompt tuning, adapters, AdapterFusion, etc.) chosen by router.</li>
        <li><b>Verifier + targeted repair</b>: don’t re-run the entire response; regenerate only broken fields/sections under constraints.</li>
        <li><b>Distillation loop</b>: log the “expensive path” and distill into cheap routers/verifiers/skills over time.</li>
      </ul>
      <div class="small">The rest of this document is a “Technique Atlas” that turns each lever into a runtime slot.</div>
    </section>

    <section id="slots">
      <h2>1) Runtime Slots (every esoteric technique has a place)</h2>
      <div class="two">
        <div>
          <h3>Request path slots</h3>
          <ul>
            <li><b>Intent router</b> (rules/embeddings/zero‑shot/tiny/LLM)</li>
            <li><b>Budget selector</b> (latency/cost/strictness)</li>
            <li><b>Plan compiler</b> (Langflow → DAG)</li>
            <li><b>Scheduler</b> (parallelism + speculative branches + cancel)</li>
            <li><b>Tool caller</b> (typed tools + retries + TTL caches)</li>
            <li><b>Retriever</b> (vector/lexical/web/graph/hybrid)</li>
            <li><b>Embedding index selector</b> (static vs. live embeddings + version choice)</li>
            <li><b>Context compressor</b> (prompt condensation + chunk distill)</li>
            <li><b>Generator</b> (model + adapter + decode policy)</li>
            <li><b>Media generator</b> (diffusion/image/video + safety filters)</li>
            <li><b>Verifier</b> (schema + factuality + constraints)</li>
            <li><b>Repair engine</b> (targeted regeneration)</li>
          </ul>
        </div>
        <div>
          <h3>Learning/control slots</h3>
          <ul>
            <li><b>Bandit / policy</b> chooses route/graph/model/adapter/decode mode</li>
            <li><b>Distiller</b> trains tiny routers/verifiers from logs</li>
            <li><b>Eval harness</b> regression + shadow tests</li>
            <li><b>Static embeddings pipeline</b> precompute/refresh/versioned indexes</li>
            <li><b>Safety/governance</b> per pack/profile allowlists & redaction</li>
            <li><b>Observability</b> traces + cache hit rates + repair rates</li>
            <li><b>Knowledge updater</b> retrieval refresh OR model editing OR adapter update</li>
          </ul>
        </div>
      </div>
    </section>

    <section id="speed">
      <h2>2) Speed Tricks (systems + inference)</h2>
      <span class="pill">Goal: avoid or shorten decoding</span>
      <h3>2.1 Avoid LLM calls</h3>
      <ul>
        <li>Cache: router decisions, tool results, embeddings, partial artifacts.</li>
        <li>Deterministic transforms: regex parsers, schema mappers, calculators.</li>
        <li>Small models first: tiny classifier/router; only escalate if uncertain.</li>
      </ul>

      <h3>2.2 Make LLM calls cheaper</h3>
      <ul>
        <li><b>Prompt compression</b> before calling LLMs (LLMLingua/LongLLMLingua style).</li>
        <li><b>Prefix/KV caching</b>: reuse stable system prompt prefixes; radix KV reuse.</li>
        <li><b>Batching + scheduling</b>: continuous batching to increase throughput.</li>
        <li><b>Disaggregated serving</b>: separate prefill from decode resources so TTFT stays low under load.</li>
        <li><b>Quantization</b>: 4-bit/8-bit inference where acceptable; for training use QLoRA-like methods.</li>
      </ul>

      <h3>2.3 Advanced inference acceleration (the “esoteric” stuff)</h3>
      <ul>
        <li><b>Speculative decoding</b>: draft model proposes tokens; larger model verifies (big latency wins for templated outputs).</li>
        <li><b>Multi-token heads</b> (Medusa-style): predict multiple future tokens per step (when supported).</li>
        <li><b>Token pruning/merging</b>: reduce effective sequence length (token merging ideas exist broadly; ToMe is one example for transformers).</li>
        <li><b>Alternate long-context architectures</b>: state-space models like Mamba can be better for some long-sequence regimes than pure attention.</li>
      </ul>
      <div class="small">
        Note: not every trick applies to every model/runtime. The point is: your runtime picks a decode strategy per request/profile.
      </div>

      <h3>2.4 Static embeddings + media acceleration</h3>
      <ul>
        <li><b>Static embeddings</b>: precompute embeddings offline; serve from versioned indexes to avoid per-request embedding cost.</li>
        <li><b>Incremental refresh</b>: only re-embed changed documents; keep backward-compatible index snapshots.</li>
        <li><b>Diffusion quantization</b>: serve 8-bit/4-bit diffusion variants when quality allows; cache latents and safety-filter outputs.</li>
      </ul>
    </section>

    <section id="retrieval">
      <h2>3) Retrieval & Memory Tricks (beyond basic RAG)</h2>
      <span class="pill">Goal: higher factuality, less hallucination, better long-doc reasoning</span>

      <h3>3.1 Retrieval strategies</h3>
      <ul>
        <li><b>Hybrid retrieval</b>: lexical + dense + reranker; pick per intent.</li>
        <li><b>Multi-query generation</b>: generate multiple query views; fuse results (RAG‑Fusion-style).</li>
        <li><b>Hierarchical / tree retrieval</b>: build abstraction trees over docs; retrieve at multiple levels (RAPTOR-style).</li>
        <li><b>Zero-shot dense retrieval pivoting</b>: generate “hypothetical docs” then embed to retrieve (HyDE-style).</li>
      </ul>

      <h3>3.2 Retrieval that self-corrects (big quality win)</h3>
      <ul>
        <li><b>Retrieve only if needed</b> and critique retrieved evidence (Self‑RAG-style).</li>
        <li><b>Retrieval evaluator</b> that detects bad retrieval and triggers alternate actions like web search (CRAG-style).</li>
      </ul>

      <h3>3.3 Memory layers</h3>
      <ul>
        <li><b>Short memory</b>: session cache (recent tool outputs, user prefs) with TTL.</li>
        <li><b>Long memory</b>: vector store + summary store + structured facts store.</li>
        <li><b>Memory compaction</b>: periodic summarization + key-value extraction (store facts, not transcripts).</li>
        <li><b>Event sourcing</b>: store “facts changed” so downstream tools can react.</li>
      </ul>

      <h3>3.4 Embedding model lifecycle (static + trainable)</h3>
      <ul>
        <li><b>Static embedding builds</b>: precompute embeddings, store versioned indexes, and map retrievers to index versions.</li>
        <li><b>Sentence-transformer training</b>: hard-negative mining, domain pairs, and periodic retrieval metric evals (MRR/nDCG).</li>
        <li><b>Rollout policy</b>: canary embedding indexes, rollback on recall regressions, and back-compat for cached queries.</li>
      </ul>
    </section>

    <section id="reasoning">
      <h2>4) Reasoning / Test-time Compute Tricks</h2>
      <span class="pill">Goal: spend compute only when needed; verify instead of rambling</span>

      <h3>4.1 Compute scaling by gating</h3>
      <ul>
        <li><b>Short-circuit</b>: if router confidence high + constraints simple, skip deep reasoning.</li>
        <li><b>Escalate</b>: only do multi-step reasoning when verifier predicts risk.</li>
      </ul>

      <h3>4.2 Search-based reasoning (when correctness matters)</h3>
      <ul>
        <li>Self-consistency: sample multiple solutions and vote.</li>
        <li>Tree/Graph of Thoughts style expansion: explore multiple branches then select via scorer/verifier.</li>
        <li>Tool-augmented reasoning: “plan → execute tools → synthesize → verify”.</li>
      </ul>

      <h3>4.3 Verifiers are the secret weapon</h3>
      <ul>
        <li><b>Schema verifier</b>: JSON Schema + domain rules.</li>
        <li><b>Evidence verifier</b>: require citations/quotes from retrieved passages for factual claims.</li>
        <li><b>Programmatic checks</b>: if numeric, recompute; if code, run tests; if finance, reconcile totals.</li>
        <li><b>Targeted repair</b>: regenerate only the failing fields/sections.</li>
      </ul>
    </section>

    <section id="routing">
      <h2>5) Routing / Cascades / Budgeting</h2>
      <span class="pill">Goal: cheapest correct path</span>

      <h3>5.1 Router ensemble (cheap → expensive)</h3>
      <ul>
        <li>Rules (regex/keywords) → embeddings → zero-shot classifier → tiny trained router → LLM router.</li>
        <li>All routers output: intent, confidence, profile, suggested graph, tool set, adapter_id, decode mode.</li>
      </ul>

      <h3>5.2 Model/graph cascades (huge practical win)</h3>
      <ul>
        <li><b>LLM cascade</b>: start with smaller/cheaper model; escalate only if needed (FrugalGPT-style).</li>
        <li><b>Graph cascade</b>: shallow plan first; deepen retrieval/reasoning only if verifier says it’s weak.</li>
        <li><b>Budget controller</b>: hard caps on latency/cost; must return “best effort” under constraints.</li>
      </ul>

      <h3>5.3 Online learning of routing</h3>
      <ul>
        <li>Contextual bandits select: route, model tier, retrieval depth, adapter, decode policy, verifier strictness.</li>
        <li>Shadow mode tests new routes without affecting user output; promote when metrics pass.</li>
      </ul>
    </section>

    <section id="peft">
      <h2>6) Specialization beyond LoRA (the PEFT “zoo”)</h2>
      <span class="pill">Goal: make skills cheap and swappable</span>

      <h3>6.1 PEFT families</h3>
      <ul>
        <li><b>Adapters</b>: insert small modules; swap per skill; can compose.</li>
        <li><b>LoRA family</b>: low-rank updates; good general default.</li>
        <li><b>Quantized LoRA training</b>: QLoRA-style 4-bit base with trained adapters.</li>
        <li><b>(IA)³ / scaling-based methods</b>: very parameter-efficient alternatives.</li>
        <li><b>Prompt/prefix tuning</b>: train “soft prompts” instead of weights.</li>
        <li><b>BitFit</b>: update biases only (sometimes works surprisingly well).</li>
        <li><b>Adapter composition</b>: fuse multiple task adapters (AdapterFusion-style) for multi-domain behavior.</li>
      </ul>

      <h3>6.2 Skill library design</h3>
      <ul>
        <li>Each adapter is a <b>Skill</b> with: intent tags, strength, safety profile, eval suite, and fallback chain.</li>
        <li>Router chooses skill; verifier monitors drift; distiller suggests new skills from frequent “expensive paths”.</li>
      </ul>

      <h3>6.3 Multi‑LoRA serving mechanics</h3>
      <ul>
        <li><b>Adapter registry</b>: store base-model compatibility, tokenizer versions, and quantization constraints.</li>
        <li><b>Hot‑load + eviction</b>: LRU/usage-based cache for adapters; merge cache for frequent compositions.</li>
        <li><b>Memory budgets</b>: enforce per-request adapter limits with fallback skills when GPU memory is tight.</li>
      </ul>

      <div class="callout">
        <b>Key idea:</b> PEFT is not just “fine-tuning”. It’s your runtime’s ability to <b>hot-swap expertise</b> per request.
      </div>
    </section>

    <section id="editing">
      <h2>7) Updating Knowledge (model editing vs retrieval)</h2>
      <span class="pill">Goal: keep system correct as reality changes</span>

      <h3>7.1 Retrieval-first (usually safest)</h3>
      <ul>
        <li>Keep facts in sources-of-truth; update indexes; let retrieval reflect reality.</li>
        <li>Use CRAG/Self-RAG style evaluation to avoid “bad retrieval poisoning”.</li>
      </ul>

      <h3>7.2 Model editing (surgical but risky)</h3>
      <ul>
        <li>Methods like ROME/MEMIT aim to edit specific factual associations in weights.</li>
        <li>Use when: offline deployments, privacy constraints, or when retrieval is impossible.</li>
        <li>Always gate with heavy evals; keep rollback/versioning; prefer edit-on-small models or adapter layers.</li>
      </ul>

      <h3>7.3 Middle ground</h3>
      <ul>
        <li>Store updated facts in structured memory; teach router to always retrieve for volatile domains (news, prices, policies).</li>
        <li>Fine-tune adapters for stable domain conventions (formatting, writing style, domain heuristics), not volatile facts.</li>
      </ul>
    </section>

    <section id="training">
      <h2>8) Alignment / Preference / Feedback Learning</h2>
      <span class="pill">Goal: make behavior consistent + controllable</span>

      <h3>8.1 Preference optimization without complex RL pipelines</h3>
      <ul>
        <li>DPO-style methods: optimize from preference pairs with a simple loss.</li>
        <li>Great for making “skills” behave consistently (tone, refusal policies, structure).</li>
      </ul>

      <h3>8.2 RLAIF + synthetic feedback loops</h3>
      <ul>
        <li>Use LLM-as-a-judge for scalable scoring; calibrate with periodic human audits.</li>
        <li>Turn failures into training data: router errors, verifier repairs, retrieval misses.</li>
      </ul>

      <h3>8.3 Safety + governance alignment</h3>
      <ul>
        <li>Pack-level allowlists, tool permissions, redaction policies, audit logs.</li>
        <li>Different safety modes per profile: strict (enterprise) vs relaxed (creative).</li>
      </ul>
    </section>

    <section id="eval">
      <h2>9) Evaluation & Observability (the “intelligence flywheel”)</h2>
      <h3>9.1 What to log (minimum viable)</h3>
      <ul>
        <li>Intent + confidence + chosen route + chosen model/skill + chosen graph</li>
        <li>Latency per node + retries + cache hit/miss</li>
        <li>Verifier failures + repair iterations</li>
        <li>Cost tokens + tool cost + total cost</li>
      </ul>

      <h3>9.2 How to evaluate</h3>
      <ul>
        <li>Regression suites per App Pack (golden inputs + required outputs).</li>
        <li>Shadow runs: test new router/graph/model without affecting production output.</li>
        <li>Judge models: use as one signal, not the only truth (calibrate periodically).</li>
        <li>Distributed evals: scale large test suites with batch compute (Dask/Ray/Spark-style).</li>
      </ul>

      <h3>9.3 Distillation loop</h3>
      <ul>
        <li>Collect traces of expensive successful paths.</li>
        <li>Distill into tiny routers/verifiers and/or new skills (adapters).</li>
        <li>Re-run eval; promote when p95 latency improves without quality loss.</li>
      </ul>
    </section>

    <section id="impl">
      <h2>10) Langflow + Cloudflare + HuggingFace mapping (concrete)</h2>
      <div class="two">
        <div>
          <h3>Cloudflare (control plane + edge runtime)</h3>
          <ul>
            <li>Workers: API gateway, request normalization, SSE streaming</li>
            <li>Durable Objects: session state, in-flight dedupe locks, orchestration state</li>
            <li>Queues: background tool runs, retriever indexing, eval jobs</li>
            <li>KV: hot caches (router results, small tool outputs)</li>
            <li>R2: artifacts, traces, compiled plans, large tool dumps</li>
          </ul>
        </div>
        <div>
          <h3>HuggingFace (model + tools plane)</h3>
          <ul>
            <li>Transformers + PEFT: skill adapters (LoRA/QLoRA/IA³/prompt tuning)</li>
            <li>Text classification: zero-shot classifier nodes</li>
            <li>Embeddings + vector DB: retrieval (dense + hybrid)</li>
            <li>Rerankers: cross-encoders or late-interaction</li>
            <li>Diffusers: quantized diffusion pipelines for image/video generation</li>
            <li>Inference endpoints / self-host: choose based on latency needs</li>
          </ul>
        </div>
      </div>

      <h3>Batch / Distributed Compute (scaling plane)</h3>
      <ul>
        <li>Dask/Ray/Spark-style jobs for embedding refresh, indexing, and large eval runs.</li>
        <li>Autoscaling pools for expensive retraining or backfill tasks; cost caps and quotas.</li>
        <li>Artifact outputs stored in R2/vector DB with versioned metadata.</li>
      </ul>

      <h3>Langflow (graph design → compiled DAG)</h3>
      <ul>
        <li>Design flows visually, but <b>compile</b> them into deterministic plans: parallel groups, caches, cancellation, and budgets.</li>
        <li>Add standard nodes: RouterEnsemble, RetrievalEvaluator, PromptCompressor, Verifier, Repair, ArtifactWriter.</li>
      </ul>
    </section>

    <section id="stock">
      <h2>11) StockCommand overlay (example App Pack)</h2>
      <ul>
        <li><b>Intents</b>: screeners, news digest, thesis builder, valuation, risk overlay, divergence/technical, portfolio actions.</li>
        <li><b>Retrieval</b>: hybrid (filings/news/earnings + web), plus CRAG-style evaluator to detect bad retrieval.</li>
        <li><b>Compute gating</b>: shallow path for “quote/summary”; deep path for “thesis/valuation”.</li>
        <li><b>Skills</b>: adapters for finance writing style + schema outputs (not volatile facts).</li>
        <li><b>Verifiers</b>: numeric recomputation; citation requirements for factual statements.</li>
        <li><b>Artifacts</b>: CSV tables, charts, PDFs/HTML reports stored in R2 and referenced back.</li>
      </ul>
    </section>

    <section>
      <h2>References (starter set)</h2>
      <ul>
        <li>LLMLingua prompt compression (arXiv:2310.05736)</li>
        <li>Self-RAG (arXiv:2310.11511)</li>
        <li>CRAG (arXiv:2401.15884)</li>
        <li>RAG-Fusion (arXiv:2402.03367)</li>
        <li>RAPTOR (arXiv:2401.18059)</li>
        <li>HyDE (arXiv:2212.10496)</li>
        <li>FrugalGPT cascades (arXiv:2305.05176)</li>
        <li>QLoRA (arXiv:2305.14314)</li>
        <li>Switch Transformers (arXiv:2101.03961)</li>
        <li>Mamba SSM (arXiv:2312.00752)</li>
        <li>DPO preference optimization (arXiv:2305.18290)</li>
      </ul>
      <div class="small">
        This document is designed to be implementation-oriented. If you want, the next iteration can expand each reference into “how to implement in your stack” code snippets + Langflow node specs.
      </div>
    </section>
  </div>
</main>

<footer>
  Built for: Langflow + Cloudflare + HuggingFace “universal app runtime”.<br/>
  Use: treat every technique as a runtime-selectable slot; then learn routing over time.
</footer>
</body>
</html>
